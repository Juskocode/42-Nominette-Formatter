{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Notebok prepared by InÃªs Gomes (ines.gomes@fe.up.pt)\\n\",\n",
    "    \"\\n\",\n",
    "    \"based on Carlos Soares (csoares@fe.up.pt) tutorial.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Recommender Systems Libraries:\\n\",\n",
    "    \"- for Rating Interactions: Surprise (https://surpriselib.com/)\\n\",\n",
    "    \"- for Implicit Interactions: Implicit (https://github.com/benfred/implicit/tree/main)\\n\",\n",
    "    \"- for an hybrid implementation: LightFM (https://making.lyst.com/lightfm/docs/index.html#)\\n\",\n",
    "    \"\\n\",\n",
    "    \"Working on Python 3.12 environment.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Another example: https://www.kaggle.com/code/gspmoreira/recommender-systems-in-python-101 \\n\",\n",
    "    \"\\n\",\n",
    "    \"Surprise does not work properly with pip install.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Table of Contents\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. [Dataset](#dataset)\\n\",\n",
    "    \"\\n\",\n",
    "    \"2. [User-item Matrix](#matrix)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    2.1 [Pandas Version](#pandas)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    2.2 [Surprise Lib Version](#surprise)\\n\",\n",
    "    \"\\n\",\n",
    "    \"3. [Recommender Systems](#recsys)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    3.1 [Popularity](#popularity)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    3.2 [Modelling](#modelling)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    3.3 [Top Recommendations](#top-rec)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from surprise import Dataset, Reader, NormalPredictor, KNNBasic, KNNWithZScore, KNNWithMeans, KNNWithZScore, SVD\\n\",\n",
    "    \"from surprise.model_selection import train_test_split, cross_validate\\n\",\n",
    "    \"from collections import defaultdict\\n\",\n",
    "    \"from surprise import accuracy\\n\",\n",
    "    \"import random\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sample dataframes\\n\",\n",
    "    \"df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\\n\",\n",
    "    \"                    'B': ['B0', 'B1', 'B2']})\\n\",\n",
    "    \"\\n\",\n",
    "    \"df2 = pd.DataFrame({'X': ['X0', 'X1'],\\n\",\n",
    "    \"                    'Y': ['Y0', 'Y1']})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Performing a Cartesian product (cross-join)\\n\",\n",
    "    \"result = df1.assign(key=1).merge(df2.assign(key=1), on='key').drop('key', axis=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(result)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import matplotlib.pyplot as plt\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## \\n\",\n",
    "    \"\\n\",\n",
    "    \"Jester dataset: https://eigentaste.berkeley.edu/dataset/\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load data\\n\",\n",
    "    \"from surprise import Dataset\\n\",\n",
    "    \"from surprise import Reader\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the Jester5k data\\n\",\n",
    "    \"data = Dataset.load_builtin('ml-100k') #ml-100k\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Our dataset is currently encapsulated in an object named \\\"dataset\\\" from the Surprise library.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"We will transform the dataset into a pandas DataFrame in order to explore and visualize the data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# transform the surprise dataset into pandas dataframe\\n\",\n",
    "    \"df = pd.DataFrame(data.raw_ratings, columns=['user_id', 'item_id', 'rating', 'comments']).drop(columns=['comments'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"What can you tell about this dataset?\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. How many ratings do we have?\\n\",\n",
    "    \"\\n\",\n",
    "    \"2. How many users do we have?\\n\",\n",
    "    \"\\n\",\n",
    "    \"3. How many items do we have?\\n\",\n",
    "    \"\\n\",\n",
    "    \"4. What is the distribution of ratings?\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# TODO\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The first step to create a recommender system, is to transform the dataset into a user-item matrix. To that end, we must first define the \\\"user\\\", the \\\"item\\\" and the \\\"value\\\". The value can be a rating (explicit feedback) or binary information (implicit feedback).\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this case, our user is the column \\\"user\\\", the item is the \\\"item\\\" and the value is the \\\"rating\\\".\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Let's create a user-item matrix based on our pandas dataframe.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# we will use the pivot function\\n\",\n",
    "    \"df_matrix = df.pivot(index='user_id', columns='item_id', values='rating')\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_matrix\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Is this dataset sparse?\\n\",\n",
    "    \"\\n\",\n",
    "    \"To calculate the sparsity, we count the number of ratings (that is, the number of cells in the matrix that are filled) and divide by the total number of user-item pairs. To count the number of users, we can simply count the number of rows in the matrix, while to count the number of items, we can simply count the number of columns. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(f\\\"{df_matrix.notnull().sum().sum() / (df_matrix.shape[0] * df_matrix.shape[1]):.2%}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"What is the:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. distribution of total number of items per user?\\n\",\n",
    "    \"\\n\",\n",
    "    \"2. distribution of total number of users per item?\\n\",\n",
    "    \"\\n\",\n",
    "    \"3. distribution of mean ratings per user?\\n\",\n",
    "    \"\\n\",\n",
    "    \"4. distribution of ratings?\\n\",\n",
    "    \"\\n\",\n",
    "    \"(Show the histograms)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# TODO\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Now let's tranform dataset in a user-item matrix **using the surprise library**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"To that end, we can apply the method \\\"build_full_trainset\\\".\\n\",\n",
    "    \"\\n\",\n",
    "    \"This \\\"trainset\\\" builds a dataset that can be used for training purposes. So be aware, that in this case we are building the \\\"training set\\\" with the full matrix!! (without train test split - keep doing the exercises to find the solution) \\n\",\n",
    "    \"\\n\",\n",
    "    \"This is the documentation for the trainset object https://surprise.readthedocs.io/en/stable/trainset.html\\n\",\n",
    "    \"\\n\",\n",
    "    \"(Using the trainset object is useful for applying the surprise library methods)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Build the trainset\\n\",\n",
    "    \"trainset = data.build_full_trainset()\\n\",\n",
    "    \"\\n\",\n",
    "    \"trainset\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Explore the trainset object. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Can you answer the same questions about the dataset using only the methods available in the trainset object? Do you have the same results?\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# TODO\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"What are the two most popular items?\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# TODO\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# HACK: the dataset is too big, so we will pick our pandas dataframe, create a sample and then covert to the surprise lib dataset\\n\",\n",
    "    \"reader = Reader(rating_scale=(df.rating.min(), df.rating.max()))\\n\",\n",
    "    \"# you can try other sizes\\n\",\n",
    "    \"size = 10000\\n\",\n",
    "    \"data_sml = Dataset.load_from_df(df[['user_id', 'item_id', 'rating']].sample(size), reader)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"To properly evaluate the recommender systems, we will now split the original dataset into train and test. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# use only the first half of the dataset as our dataset\\n\",\n",
    "    \"# split intro train and test set\\n\",\n",
    "    \"trainset, testset = train_test_split(data, test_size=0.2)\\n\",\n",
    "    \"# if you want to use the small dataset, please change data to data_sml\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The popularity method is the simplest recommender system.\\n\",\n",
    "    \"\\n\",\n",
    "    \"It finds the most popular items and then recommends them to new users.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Popular Recommender -> maybe we should use the rating too\\n\",\n",
    "    \"def popular_recommendations(trainset, top_n=10):\\n\",\n",
    "    \"    item_counts = defaultdict(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Iterate through the trainset to count item ratings\\n\",\n",
    "    \"    for _, item_id, _ in trainset.all_ratings():\\n\",\n",
    "    \"        item_counts[item_id] += 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Sort items by popularity (number of ratings)\\n\",\n",
    "    \"    popular_items = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Get the top N most popular items (e.g., top 10)\\n\",\n",
    "    \"    top_n = popular_items[:top_n]\\n\",\n",
    "    \"    return [trainset.to_raw_iid(i) for i, _ in top_n]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"These are the most popular items\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"popular_recommendations(trainset, 5)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The predictor fills the matrix with the ratings por unseen user-item pairs.\\n\",\n",
    "    \"\\n\",\n",
    "    \"To evaluate how good the model is, we calculate the RMSE between the true value and the predicted. \\n\",\n",
    "    \"\\n\",\n",
    "    \"As we are using the test set to create the predictions only for user-item pairs found in the test set.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define evaluation function\\n\",\n",
    "    \"def evaluate_algorithm(algo, trainset, testset):\\n\",\n",
    "    \"    algo.fit(trainset)\\n\",\n",
    "    \"    predictions = algo.test(testset)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Compute and return RMSE\\n\",\n",
    "    \"    rmse = accuracy.rmse(predictions)\\n\",\n",
    "    \"    return rmse\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Random Recommender\\n\",\n",
    "    \"random_algo = NormalPredictor()\\n\",\n",
    "    \"random_rmse = evaluate_algorithm(random_algo, trainset, testset)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# User-Based Collaborative Filtering\\n\",\n",
    "    \"#ubcf_algo = KNNBasic(sim_options={'user_based': True})\\n\",\n",
    "    \"#ubcf_rmse = evaluate_algorithm(ubcf_algo, trainset, testset)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Item-Based Collaborative Filtering\\n\",\n",
    "    \"ibcf_algo = KNNBasic(sim_options={'user_based': False})\\n\",\n",
    "    \"ibcf_rmse = evaluate_algorithm(ibcf_algo, trainset, testset)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Singular Value Decomposition (SVD)\\n\",\n",
    "    \"svd_algo = SVD()\\n\",\n",
    "    \"svd_rmse = evaluate_algorithm(svd_algo, trainset, testset)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(f\\\"Random RMSE: {random_rmse:.3f}\\\")\\n\",\n",
    "    \"#print(f\\\"User-Based CF RMSE: {ubcf_rmse:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Item-Based CF RMSE: {ibcf_rmse:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"SVD RMSE: {svd_rmse:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The following function is designed to generate personalized recommendations for a user using a recommender model (`algo`) and a `Trainset` object. It uses the recommender model to make a rating prediction for each item the user hasn't interacted with and sorts the items by their estimated scores in descending order. Then, selects the top `n` items with the highest estimated scores as recommendations for the user.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Recommend top N items for a user using a recommender model\\n\",\n",
    "    \"def recommend_top_n(algo, trainset, user_id, n=10):\\n\",\n",
    "    \"    user_ratings = trainset.ur[user_id]\\n\",\n",
    "    \"    items = [item_id for (item_id, _) in user_ratings]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    item_scores = {}\\n\",\n",
    "    \"    # this is actually not the most correct way to do this, but it works\\n\",\n",
    "    \"    for item_id in trainset.all_items():\\n\",\n",
    "    \"        if item_id not in items:\\n\",\n",
    "    \"            prediction = algo.predict(trainset.to_raw_uid(user_id), trainset.to_raw_iid(item_id), verbose=True)\\n\",\n",
    "    \"            item_scores[item_id] = prediction.est\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    top_items = sorted(item_scores, key=item_scores.get, reverse=True)[:n]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    #from raw_id to actual_id\\n\",\n",
    "    \"    return [trainset.to_raw_iid(i) for i in top_items]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Get recommendations for a specific user using the User-Based CF model\\n\",\n",
    "    \"user_id = 3 # Change to the desired user ID\\n\",\n",
    "    \"ubcf_top_items = recommend_top_n(ibcf_algo, trainset, user_id , n=5)\\n\",\n",
    "    \"print(\\\"Top 5 User-Based CF Recommendations for User\\\", trainset.to_raw_uid(user_id), \\\":\\\", ubcf_top_items)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"user_id = 10\\n\",\n",
    "    \"n = 5\\n\",\n",
    "    \"print(\\\"user_id\\\", trainset.to_raw_uid(user_id))\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(recommend_top_n(svd_algo, trainset, user_id, n))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"How to evaluate this ranking?\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_testset = pd.DataFrame(testset, columns=['user_id', 'item_id', 'rating'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# we just want to recommend positive ratings\\n\",\n",
    "    \"pos_rating = 5\\n\",\n",
    "    \"df_testset_pos = df_testset[df_testset[\\\"rating\\\"] >= pos_rating]\\n\",\n",
    "    \"# which users exist in the training and testset\\n\",\n",
    "    \"users = []\\n\",\n",
    "    \"for u in df_testset_pos[\\\"user_id\\\"].unique():\\n\",\n",
    "    \"    try :\\n\",\n",
    "    \"        trainset.to_inner_uid(u)\\n\",\n",
    "    \"        users.append(u)\\n\",\n",
    "    \"    except ValueError:\\n\",\n",
    "    \"        continue\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"number of users in the testset that exist in the trainset:\\\", len(users))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"random_user = random.choice(users)\\n\",\n",
    "    \"n = 5\\n\",\n",
    "    \"print(\\\"user_id : \\\", random_user)\\n\",\n",
    "    \"gt = df_testset[(df_testset['user_id']==random_user) & (df_testset['rating']>pos_rating)].item_id.to_list()\\n\",\n",
    "    \"print(\\\"ground truth : \\\", gt)\\n\",\n",
    "    \"recs =  recommend_top_n(svd_algo, trainset, trainset.to_inner_uid(random_user), n)\\n\",\n",
    "    \"print(\\\"recommendations: \\\",recs)\\n\",\n",
    "    \"print(f\\\"hits: {len(set(gt).intersection(set(recs)))} / {n}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"adco_env_3.12\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.12.6\"\n",
    "  },\n",
    "  \"orig_nbformat\": 4\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Dataset](#dataset)\n",
    "\n",
    "2. [User-item Matrix](#matrix)\n",
    "\n",
    "    2.1 [Pandas Version](#pandas)\n",
    "\n",
    "    2.2 [Surprise Lib Version](#surprise)\n",
    "\n",
    "3. [Recommender Systems](#recsys)\n",
    "\n",
    "    3.1 [Popularity](#popularity)\n",
    "\n",
    "    3.2 [Modelling](#modelling)\n",
    "\n",
    "    3.3 [Top Recommendations](#top-rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, NormalPredictor, KNNBasic, KNNWithZScore, KNNWithMeans, KNNWithZScore, SVD\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "from collections import defaultdict\n",
    "from surprise import accuracy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataframes\n",
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
    "                    'B': ['B0', 'B1', 'B2']})\n",
    "\n",
    "df2 = pd.DataFrame({'X': ['X0', 'X1'],\n",
    "                    'Y': ['Y0', 'Y1']})\n",
    "\n",
    "# Performing a Cartesian product (cross-join)\n",
    "result = df1.assign(key=1).merge(df2.assign(key=1), on='key').drop('key', axis=1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "Jester dataset: https://eigentaste.berkeley.edu/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "# Load the Jester5k data\n",
    "data = Dataset.load_builtin('ml-100k') #ml-100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is currently encapsulated in an object named \"dataset\" from the Surprise library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will transform the dataset into a pandas DataFrame in order to explore and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the surprise dataset into pandas dataframe\n",
    "df = pd.DataFrame(data.raw_ratings, columns=['user_id', 'item_id', 'rating', 'comments']).drop(columns=['comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you tell about this dataset?\n",
    "\n",
    "1. How many ratings do we have?\n",
    "\n",
    "2. How many users do we have?\n",
    "\n",
    "3. How many items do we have?\n",
    "\n",
    "4. What is the distribution of ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to create a recommender system, is to transform the dataset into a user-item matrix. To that end, we must first define the \"user\", the \"item\" and the \"value\". The value can be a rating (explicit feedback) or binary information (implicit feedback).\n",
    "\n",
    "In this case, our user is the column \"user\", the item is the \"item\" and the value is the \"rating\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a user-item matrix based on our pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the pivot function\n",
    "df_matrix = df.pivot(index='user_id', columns='item_id', values='rating')\n",
    "\n",
    "df_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this dataset sparse?\n",
    "\n",
    "To calculate the sparsity, we count the number of ratings (that is, the number of cells in the matrix that are filled) and divide by the total number of user-item pairs. To count the number of users, we can simply count the number of rows in the matrix, while to count the number of items, we can simply count the number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df_matrix.notnull().sum().sum() / (df_matrix.shape[0] * df_matrix.shape[1]):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the:\n",
    "\n",
    "1. distribution of total number of items per user?\n",
    "\n",
    "2. distribution of total number of users per item?\n",
    "\n",
    "3. distribution of mean ratings per user?\n",
    "\n",
    "4. distribution of ratings?\n",
    "\n",
    "(Show the histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tranform dataset in a user-item matrix **using the surprise library**.\n",
    "\n",
    "To that end, we can apply the method \"build_full_trainset\".\n",
    "\n",
    "This \"trainset\" builds a dataset that can be used for training purposes. So be aware, that in this case we are building the \"training set\" with the full matrix!! (without train test split - keep doing the exercises to find the solution) \n",
    "\n",
    "This is the documentation for the trainset object https://surprise.readthedocs.io/en/stable/trainset.html\n",
    "\n",
    "(Using the trainset object is useful for applying the surprise library methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the trainset\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the trainset object. \n",
    "\n",
    "Can you answer the same questions about the dataset using only the methods available in the trainset object? Do you have the same results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the two most popular items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: the dataset is too big, so we will pick our pandas dataframe, create a sample and then covert to the surprise lib dataset\n",
    "reader = Reader(rating_scale=(df.rating.min(), df.rating.max()))\n",
    "# you can try other sizes\n",
    "size = 10000\n",
    "data_sml = Dataset.load_from_df(df[['user_id', 'item_id', 'rating']].sample(size), reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly evaluate the recommender systems, we will now split the original dataset into train and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only the first half of the dataset as our dataset\n",
    "# split intro train and test set\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "# if you want to use the small dataset, please change data to data_sml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The popularity method is the simplest recommender system.\n",
    "\n",
    "It finds the most popular items and then recommends them to new users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popular Recommender -> maybe we should use the rating too\n",
    "def popular_recommendations(trainset, top_n=10):\n",
    "    item_counts = defaultdict(int)\n",
    "\n",
    "    # Iterate through the trainset to count item ratings\n",
    "    for _, item_id, _ in trainset.all_ratings():\n",
    "        item_counts[item_id] += 1\n",
    "\n",
    "    # Sort items by popularity (number of ratings)\n",
    "    popular_items = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N most popular items (e.g., top 10)\n",
    "    top_n = popular_items[:top_n]\n",
    "    return [trainset.to_raw_iid(i) for i, _ in top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most popular items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_recommendations(trainset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor fills the matrix with the ratings por unseen user-item pairs.\n",
    "\n",
    "To evaluate how good the model is, we calculate the RMSE between the true value and the predicted. \n",
    "\n",
    "As we are using the test set to create the predictions only for user-item pairs found in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_algorithm(algo, trainset, testset):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    \n",
    "    # Compute and return RMSE\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Recommender\n",
    "random_algo = NormalPredictor()\n",
    "random_rmse = evaluate_algorithm(random_algo, trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Based Collaborative Filtering\n",
    "#ubcf_algo = KNNBasic(sim_options={'user_based': True})\n",
    "#ubcf_rmse = evaluate_algorithm(ubcf_algo, trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-Based Collaborative Filtering\n",
    "ibcf_algo = KNNBasic(sim_options={'user_based': False})\n",
    "ibcf_rmse = evaluate_algorithm(ibcf_algo, trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "svd_algo = SVD()\n",
    "svd_rmse = evaluate_algorithm(svd_algo, trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Random RMSE: {random_rmse:.3f}\")\n",
    "#print(f\"User-Based CF RMSE: {ubcf_rmse:.3f}\")\n",
    "print(f\"Item-Based CF RMSE: {ibcf_rmse:.3f}\")\n",
    "print(f\"SVD RMSE: {svd_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is designed to generate personalized recommendations for a user using a recommender model (`algo`) and a `Trainset` object. It uses the recommender model to make a rating prediction for each item the user hasn't interacted with and sorts the items by their estimated scores in descending order. Then, selects the top `n` items with the highest estimated scores as recommendations for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend top N items for a user using a recommender model\n",
    "def recommend_top_n(algo, trainset, user_id, n=10):\n",
    "    user_ratings = trainset.ur[user_id]\n",
    "    items = [item_id for (item_id, _) in user_ratings]\n",
    "    \n",
    "    item_scores = {}\n",
    "    # this is actually not the most correct way to do this, but it works\n",
    "    for item_id in trainset.all_items():\n",
    "        if item_id not in items:\n",
    "            prediction = algo.predict(trainset.to_raw_uid(user_id), trainset.to_raw_iid(item_id), verbose=True)\n",
    "            item_scores[item_id] = prediction.est\n",
    "    \n",
    "    top_items = sorted(item_scores, key=item_scores.get, reverse=True)[:n]\n",
    "\n",
    "    #from raw_id to actual_id\n",
    "    return [trainset.to_raw_iid(i) for i in top_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations for a specific user using the User-Based CF model\n",
    "user_id = 3 # Change to the desired user ID\n",
    "ubcf_top_items = recommend_top_n(ibcf_algo, trainset, user_id , n=5)\n",
    "print(\"Top 5 User-Based CF Recommendations for User\", trainset.to_raw_uid(user_id), \":\", ubcf_top_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 10\n",
    "n = 5\n",
    "print(\"user_id\", trainset.to_raw_uid(user_id))\n",
    "\n",
    "print(recommend_top_n(svd_algo, trainset, user_id, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to evaluate this ranking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset = pd.DataFrame(testset, columns=['user_id', 'item_id', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just want to recommend positive ratings\n",
    "pos_rating = 5\n",
    "df_testset_pos = df_testset[df_testset[\"rating\"] >= pos_rating]\n",
    "# which users exist in the training and testset\n",
    "users = []\n",
    "for u in df_testset_pos[\"user_id\"].unique():\n",
    "    try :\n",
    "        trainset.to_inner_uid(u)\n",
    "        users.append(u)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "print(\"number of users in the testset that exist in the trainset:\", len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_user = random.choice(users)\n",
    "n = 5\n",
    "print(\"user_id : \", random_user)\n",
    "gt = df_testset[(df_testset['user_id']==random_user) & (df_testset['rating']>pos_rating)].item_id.to_list()\n",
    "print(\"ground truth : \", gt)\n",
    "recs =  recommend_top_n(svd_algo, trainset, trainset.to_inner_uid(random_user), n)\n",
    "print(\"recommendations: \",recs)\n",
    "print(f\"hits: {len(set(gt).intersection(set(recs)))} / {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adco_env_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
